# Configuración de Ollama
# Desde devcontainer, usar host.docker.internal para acceder al host
# Desde host local, usar localhost
OLLAMA_HOST=http://172.17.0.1:11434
# Modelo a usar
OLLAMA_MODEL=qwen3:4b-instruct
# Temperatura del modelo (0.0 = determinístico, 1.0 = creativo)
OLLAMA_TEMPERATURE=0.7
# Modelo a utilizar para instanciar el embedding dentro del RAG
# RAG_MODEL=llama3.2:latest
RAG_MODEL=embeddinggemma:latest

# Configuración de Neo4j
NEO4J_URI=neo4j://172.17.0.1:7687
NEO4J_USER=neo4j
NEO4J_PASS=password
